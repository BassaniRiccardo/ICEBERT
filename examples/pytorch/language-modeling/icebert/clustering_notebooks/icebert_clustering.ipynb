{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "icebert_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uXibIAb0SH7k",
        "6ybR-3V0KcWh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TFuaqd1E9jy"
      },
      "source": [
        "# Icebert Clustering\n",
        "\n",
        "This notebook guides you through the creation of objects necessary to the pre-training of an ICEBERT language model, as described in [Transformers-IcebertMLM](https://github.com/BassaniRiccardo/transformers/tree/master/examples/pytorch/language-modeling).\n",
        "\n",
        "The following objects will be created at the end of the notebook:\n",
        "- cID mapper: a dictionary containing the cluster ID of each token in the vocabulary\n",
        "- Baseline tokenizer: the tokenizer of the baseline model\n",
        "- ICEBERT tokenizer: the tokenizer of the ICEBERT model\n",
        "- Monolingual tokenizers: a folder containing monolingual tokenizers to be used in the mapping-to-cIDs process\n",
        "\n",
        "*Notes*: \n",
        "- The set of languages can be modified in the \"Packages, Libraries, Variables and Methods\" section.\n",
        "- The code will retrun slightly different results at each run, due to the randomic nature of the METIS partitioning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QRYMXOEG2CT"
      },
      "source": [
        "We start by mounting the drive: the downloaded/created files will be saved there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUC7xZNBifHm",
        "outputId": "30bffa4b-d67d-4d97-bf7d-045bd0db9ef7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQbjRbqAKUVE"
      },
      "source": [
        "# 0. Packages, Libraries, Variables and Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4IhQ4ccHGgg"
      },
      "source": [
        "We define:\n",
        "- a repo folder, the root folder of this project\n",
        "- an obects folder, the root folder for the objects to save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-IFBdHv4J06"
      },
      "source": [
        "! export REPO_FOLDER=/content/drive/MyDrive/notebooks/icebert_clustering\n",
        "! export OBJECTS_FOLDER=$REPO_FOLDER/objects"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFuWvtUgrAra",
        "outputId": "1578e5df-3cdd-4fea-bc01-46ccd7a15f66"
      },
      "source": [
        "! echo $OBJECTS_FOLDER"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/notebooks/icebert_clustering/objects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p853s31zHSWL"
      },
      "source": [
        "We create the folder structure for the objects we want to create:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNCfYB2q4ZEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e7fe25-f568-4075-df66-7aed55a0dac7"
      },
      "source": [
        "# ! mkdir $OBJECTS_FOLDER\n",
        "# ! mkdir $OBJECTS_FOLDER/voc_dicts\n",
        "# ! mkdir $OBJECTS_FOLDER/voc_dicts_inv\n",
        "# ! mkdir $OBJECTS_FOLDER/embs\n",
        "# ! mkdir $OBJECTS_FOLDER/embs/monolingual\n",
        "# ! mkdir $OBJECTS_FOLDER/embs/multilingual\n",
        "# ! mkdir $OBJECTS_FOLDER/for_tokenizer_vocabs\n",
        "# ! mkdir $OBJECTS_FOLDER/translation_graphs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: missing operand\n",
            "Try 'mkdir --help' for more information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNJwSgcIHXJw"
      },
      "source": [
        "We clone the repository necessary for cross-lingual embedding mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEvcGDg_P5Vd"
      },
      "source": [
        "# % cd /content/drive/MyDrive/notebooks/icebert_clustering/\n",
        "# ! git clone https://github.com/codogogo/xling-eval.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxbGPvsIHkMZ"
      },
      "source": [
        "We install necessary packages and import necessary methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaXp7xRoO9aM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d78670e-54a9-4f0e-c63d-36141ff39e21"
      },
      "source": [
        "! pip install sentencepiece\n",
        "! pip install transformers\n",
        "! pip install pytorch_transformers\n",
        "! pip install tqdm"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.18.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.0.45)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.13 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (1.21.13)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.13->boto3->pytorch_transformers) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.13->boto3->pytorch_transformers) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.13->boto3->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmeXtNq-Q_a1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# import util\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import time\n",
        "from sys import stdin\n",
        "from collections import defaultdict\n",
        "from collections import OrderedDict\n",
        "import community\n",
        "from scipy import sparse\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# xling-eval libraries\n",
        "sys.path.append('/content/drive/MyDrive/notebooks/icebert_clustering/xling-eval/code')\n",
        "import util\n",
        "import sims\n",
        "import projection"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVBN2dEZH2Df"
      },
      "source": [
        "We define some important utility functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2lpr_qN2jgS"
      },
      "source": [
        "def inverse_dict(my_dict, unique=False):\n",
        "    result_dict = {}\n",
        "    if unique:\n",
        "      for key, value in my_dict.items():\n",
        "        result_dict[value] = key\n",
        "    else:\n",
        "      for key, value in my_dict.items():\n",
        "        if not value in result_dict.keys():\n",
        "          result_dict[value] = []\n",
        "        result_dict[value].append(key)\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "# saves an object in the 'objects' folder\n",
        "def save_obj(obj, name, folder='', protocol=pickle.HIGHEST_PROTOCOL):\n",
        "  if folder == '':\n",
        "    file_name = OBJECTS_FOLDER + name + '.pkl'\n",
        "  else:\n",
        "    file_name = OBJECTS_FOLDER + folder + '/' + name + '.pkl'\n",
        "  with open(file_name, 'wb') as f:\n",
        "    pickle.dump(obj, f, protocol)\n",
        "\n",
        "\n",
        "# loads an object from the 'objects' folder\n",
        "def load_obj(name, folder='', protocol=pickle.HIGHEST_PROTOCOL):\n",
        "  if folder == '':\n",
        "    file_name = OBJECTS_FOLDER + name + '.pkl'\n",
        "  else:\n",
        "    file_name = OBJECTS_FOLDER + folder + '/' + name + '.pkl'\n",
        "  with open(file_name, 'rb') as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "def remove_punctuation_digits_from_voc(voc, map_digits_to_zero=True):\n",
        "  \"\"\"\n",
        "  Modifies a vocabulary by removing all punctuation and digits.\n",
        "  Returns the modified voc and the list of removed punctuation and digits.\n",
        "  \"\"\"\n",
        "  new_voc = []\n",
        "  punctuation_digits = []\n",
        "  for token in voc:\n",
        "    t = effective_token(token)\n",
        "    if (not t in punctuation) and (not t.isdigit()):\n",
        "      new_voc.append(token)\n",
        "    else:\n",
        "      punctuation_digits.append(token)\n",
        "  return new_voc, punctuation_digits\n",
        "\n",
        "def effective_token(s):\n",
        "  \"\"\"\n",
        "  Returns the token ingoring the '##' markers\n",
        "  \"\"\"\n",
        "  if '##' == s[:2]:\n",
        "    return s[2:]\n",
        "  return s"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Da1sg8HoZx"
      },
      "source": [
        "We define the languages to include and some important variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3rVL4maRkf2"
      },
      "source": [
        "id_to_ln = {0: 'ar', 1: 'bn', 2: 'en', 3: 'fi', 4: 'id', 5 : 'ko', 6 : 'ru', 7 : 'sw', 8 : 'te'}  \n",
        "ln_to_id = inverse_dict(id_to_ln, unique=True)\n",
        "\n",
        "all_ln = ['ar', 'bn', 'en', 'fi', 'id', 'ko', 'ru', 'sw', 'te']\n",
        "map_ln = all_ln\n",
        "languages = ['Arabic', 'Bengali', 'English', 'Finnish', 'Indonesian' , 'Korean', 'Russian', 'Swahili', 'Telugu'] \n",
        "\n",
        "OBJECTS_FOLDER = \"/content/drive/MyDrive/notebooks/icebert_clustering/objects\"\n",
        "MON_EMBEDDINGS_ROOT = OBJECTS_FOLDER + \"/embs/monolingual/vectors_\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXibIAb0SH7k"
      },
      "source": [
        "# 1. BPEmb Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHizsv8OIlK5"
      },
      "source": [
        "In this section:\n",
        "- We download monolinugal vocabularies and embeddings\n",
        "- We save monolinugal and multilingual (mapped into a shared space) embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DvsohdirF8G"
      },
      "source": [
        "We use the BPEmb package to download monolingual embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMTji8dvSMDM"
      },
      "source": [
        "! pip install bpemb\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwvkOcsfrL21"
      },
      "source": [
        "While we directly download the vocabularies, so that we can:\n",
        "- select the size we want (30k);\n",
        "- change the subwords marker to \"##\";\n",
        "- extract the embeddings for the vocab we use;\n",
        "\n",
        "We save everything in our BPEmb folder.\n",
        "\n",
        "---\n",
        "\n",
        "We download vocabularies of size 25K and 50K, to extract 30K tokens vocabularies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2AgHE7SUodn"
      },
      "source": [
        "!wget https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/fi/fi.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/ko/ko.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/ru/ru.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/sw/sw.wiki.bpe.vs25000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/te/te.wiki.bpe.vs25000.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJP5njb_lhjt"
      },
      "source": [
        "!wget https://nlp.h-its.org/bpemb/ar/ar.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/bn/bn.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/fi/fi.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/id/id.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/ko/ko.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/ru/ru.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/sw/sw.wiki.bpe.vs50000.vocab\n",
        "!wget https://nlp.h-its.org/bpemb/te/te.wiki.bpe.vs50000.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_3IoisHrXdQ"
      },
      "source": [
        "We need:\n",
        " * Monolingual vocabularies including (multilingual) backoff characters, punctuation and digits, to be used by the tokenizer --> for_tokenizer_vocabs\n",
        " * Monolingual \"clean\" vocabulary, to be used in the clustering --> voc_dicts and voc_dicts_inv\n",
        " * Mulitlingual set of shared tokens, including:\n",
        "  * punctuation\n",
        "  * digits\n",
        "  * backoff characters\n",
        "  ---> special tokens\n",
        "\n",
        "Each language is tokenized monolingually, then:\n",
        "- each token queries the tok_to_cID dictionary\n",
        "- if the key is not present, the query is repeated after unmarking the token (to find interlingua special tokens)\n",
        "- if the key is absent, \"UNK\" cID is given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFoQG8gju1Xw"
      },
      "source": [
        "Set a limit for the number of special characters each language can have This is necessary since special characters are selected as the ones present at the end of different size vocabularies. While for most languages the set of such characters has a reasonably low size, for some languages like Begnali or Korean special character would be too many, and the final vocabulary would be of bad quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0oIEMvc3_sW"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "special_chars_limit = {}\n",
        "for ln in all_ln:\n",
        "  special_chars_limit[ln] = 500\n",
        "special_chars_limit[\"bn\"] = 216\n",
        "special_chars_limit[\"ko\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OC_SId6u7tO"
      },
      "source": [
        "We extract a vocabulary of size 30k from a voc of size 50k and one of size 25k:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1736zFCZwoc"
      },
      "source": [
        "def extract_vocab_dict_old(ln, size=30000, include_characters=False, voc_fil_path=\"/content/\", essential_chars=punctuation):\n",
        "  \"\"\"\n",
        "  Access two text files containing two different sized BPEmb dictionaries.\n",
        "  Select the desired number of tokens, keeping the special characters at the end of the text files.\n",
        "  Requires the two files fo size SMALL and BIG to be in the - voc_fil_path - directory.\n",
        "  Allows to extract a vocabulary of size s.t. SMALL < size < BIG\n",
        "  \"\"\"\n",
        "  complete_voc = []\n",
        "  voc_dict = {}\n",
        "  voc_dict_inv = {}\n",
        "  # to retrieve the embeddings\n",
        "  tok_to_index = {}\n",
        "  with open (voc_fil_path +ln + \".wiki.bpe.vs25000.vocab\", 'r') as voc_25, open(voc_fil_path +ln + \".wiki.bpe.vs50000.vocab\", 'r') as voc_50:\n",
        "    # select the desired number of lines\n",
        "    lines_25 = voc_25.readlines()\n",
        "    lines_50 = voc_50.readlines()\n",
        "    useful_lines = []\n",
        "    first_useful_index = 3\n",
        "    # find backoff characters (including some punctuation): they are shared by all-size vocs at the end\n",
        "    backoff_characters = []\n",
        "    special_chars_index = -1\n",
        "    while (lines_25[special_chars_index].split()[0] == lines_50[special_chars_index].split()[0]):\n",
        "      token = lines_50[special_chars_index].split()[0]\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      backoff_characters.append(token)\n",
        "      special_chars_index -= 1\n",
        "    # keeping the special characters at the end of the text file\n",
        "    if include_characters: \n",
        "      useful_lines.extend(backoff_characters)\n",
        "    # if too many backoff characters, keep them as normal characters\n",
        "    if not include_characters and len(backoff_characters) > special_chars_limit[ln]:\n",
        "      useful_lines.extend(lines_50[special_chars_index : -special_chars_limit[ln]])          # all tyhe tokens beyond then max number are considered normal\n",
        "    # or excluding them\n",
        "    useful_lines.extend(lines_50[first_useful_index : first_useful_index + size])\n",
        "\n",
        "    # add each token to the vocabulary\n",
        "    for enum, line in enumerate(useful_lines):\n",
        "      try:\n",
        "        token, index = line.split()\n",
        "      except:\n",
        "        print(line)\n",
        "        print(enum + first_useful_index)\n",
        "        return\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      # make sure there are not duplicates due to the digits substitution\n",
        "      if not token in complete_voc:\n",
        "        complete_voc.append(token)\n",
        "        tok_to_index[token] = int(index[1:]) + 3\n",
        "\n",
        "  # remove punctuation and digits, and return a dictionary tok->id and one id->tok. Also return pd \n",
        "  voc_no_pd, special = MergeVoc.remove_punctuation_digits_from_voc(complete_voc)\n",
        "  print(ln, \"Vocab length:\", len(voc_no_pd))\n",
        "  for i, t in enumerate(voc_no_pd):\n",
        "    voc_dict[t] = i\n",
        "    voc_dict_inv[i] = t\n",
        "\n",
        "  # also return a vocabulary including punctuation, digits and backoff chars (also the unmarked version), to be used by the tokenizer  \n",
        "  complete_voc.extend(backoff_characters)\n",
        "  # korean, or \"symbolic\" languages in general, have a lot of single characters to keep: they are kept in the voc and not as special tokens\n",
        "  if len(backoff_characters) > special_chars_limit[ln]:\n",
        "    special.extend(backoff_characters[: special_chars_limit[ln]])\n",
        "  else:\n",
        "    special.extend(backoff_characters)\n",
        "\n",
        "  # make sure all the characters are present both hashed and non-hashed\n",
        "  for c in special:\n",
        "    if c[:2] == \"##\" and c[2:] not in complete_voc:\n",
        "      special.append(c[2:])\n",
        "      complete_voc.append(c[2:])\n",
        "\n",
        "  # make sure essential characters (punctuation) are included\n",
        "  for c in essential_chars:\n",
        "    if c not in complete_voc:\n",
        "      complete_voc.append(c)\n",
        "    if c not in special:\n",
        "      special.append(c)\n",
        "\n",
        "  return voc_dict, voc_dict_inv, tok_to_index, complete_voc, special"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2sgswmArUSf"
      },
      "source": [
        "Just take the 25k vocabulary to avoid missing important tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwlRp8SNK22"
      },
      "source": [
        "def extract_vocab_dict(ln, size = 30000, voc_fil_path=\"/content/\", essential_chars=punctuation):\n",
        "  \"\"\"\n",
        "  Access two text files containing two different sized BPEmb dictionaries.\n",
        "  Select the desired number of tokens, keeping the special characters at the end of the text files.\n",
        "  Requires the two files fo size SMALL and BIG to be in the - voc_fil_path - directory.\n",
        "  Allows to extract a vocabulary of size s.t. SMALL < size < BIG\n",
        "  \"\"\"\n",
        "  complete_voc = []\n",
        "  voc_dict = {}\n",
        "  voc_dict_inv = {}\n",
        "  # to retrieve the embeddings\n",
        "  tok_to_index = {}\n",
        "  with open (voc_fil_path +ln + \".wiki.bpe.vs25000.vocab\", 'r') as voc_25, open(voc_fil_path +ln + \".wiki.bpe.vs50000.vocab\", 'r') as voc_50:\n",
        "    \n",
        "    # select the desired number of lines\n",
        "    lines_25 = voc_25.readlines()\n",
        "    lines_50 = voc_50.readlines()\n",
        "    useful_lines = []\n",
        "    first_useful_index = 3\n",
        "        \n",
        "    # get tokens indexes from the 50k voc so we can retrieve the embeddings\n",
        "    tok_to_index_50 = dict()\n",
        "    for line in lines_50[first_useful_index:]:\n",
        "      token, index = line.split()\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      tok_to_index_50[token] = int(index[1:]) + 3\n",
        "\n",
        "\n",
        "    # find backoff characters (including some punctuation): they are shared by all-size vocs at the end\n",
        "    backoff_characters = []\n",
        "    special_chars_index = -1\n",
        "    while (lines_25[special_chars_index].split()[0] == lines_50[special_chars_index].split()[0]):\n",
        "      token = lines_25[special_chars_index].split()[0]\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      backoff_characters.append(token)\n",
        "      special_chars_index -= 1\n",
        "    # exclude special chars from the tokens to cluster\n",
        "    # if too many backoff characters, keep them as normal characters\n",
        "    if len(backoff_characters) > special_chars_limit[ln]:\n",
        "      special_chars_index = -special_chars_limit[ln]\n",
        "    # select the tokens to cluster\n",
        "    useful_lines.extend(lines_25[first_useful_index : special_chars_index])   # include all tokens from the 25k vocabulary\n",
        "\n",
        "    # add each token to the vocabulary\n",
        "    for enum, line in enumerate(useful_lines):\n",
        "      try:\n",
        "        token, index = line.split()\n",
        "      except:\n",
        "        print(line)\n",
        "        print(enum + first_useful_index)\n",
        "        return\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      # make sure there are not duplicates due to the digits substitution\n",
        "      if not token in complete_voc:\n",
        "        complete_voc.append(token)\n",
        "\n",
        "    # add extra tokens to reach 30k\n",
        "    count = len(complete_voc)\n",
        "    for enum, line in enumerate(lines_50[first_useful_index:]):\n",
        "      try:\n",
        "        token, index = line.split()\n",
        "      except:\n",
        "        print(line)\n",
        "        print(enum + first_useful_index)\n",
        "        return\n",
        "      token = ''.join(\"0\" if c.isdigit() else c for c in token)\n",
        "      if token[0] == \"▁\" and not token == \"▁\":\n",
        "        token = token[1:]\n",
        "      else:\n",
        "        token = \"##\" + token\n",
        "      # make sure there are not duplicates due to the digits substitution\n",
        "      if not token in complete_voc:\n",
        "        complete_voc.append(token)\n",
        "        count +=1\n",
        "      # when the desired size is reached, terminate\n",
        "      if not count < size:\n",
        "        break\n",
        "\n",
        "  # remove punctuation and digits, and return a dictionary tok->id and one id->tok. Also return pd \n",
        "  voc_no_pd, special = remove_punctuation_digits_from_voc(complete_voc)\n",
        "  print(ln, \"Vocab length:\", len(voc_no_pd))\n",
        "  for i, t in enumerate(voc_no_pd):\n",
        "    voc_dict[t] = i\n",
        "    voc_dict_inv[i] = t\n",
        "\n",
        "  # also return a vocabulary including punctuation, digits and backoff chars (also the unmarked version), to be used by the tokenizer  \n",
        "  complete_voc.extend(backoff_characters)\n",
        "  # korean, or \"symbolic\" languages in general, have a lot of single characters to keep: they are kept in the voc and not as special tokens\n",
        "  if len(backoff_characters) > special_chars_limit[ln]:\n",
        "    special.extend(backoff_characters[: special_chars_limit[ln]])\n",
        "  else:\n",
        "    special.extend(backoff_characters)\n",
        "\n",
        "  # make sure all the special characters are present both in hashed and non-hashed form\n",
        "  for c in special:\n",
        "    if c[:2] == \"##\" and c[2:] not in complete_voc:\n",
        "      special.append(c[2:])\n",
        "      complete_voc.append(c[2:])\n",
        "\n",
        "  # make sure essential characters (punctuation) are included\n",
        "  for c in essential_chars:\n",
        "    if c not in complete_voc:\n",
        "      complete_voc.append(c)\n",
        "    if c not in special:\n",
        "      special.append(c)\n",
        "\n",
        "  return voc_dict, voc_dict_inv, tok_to_index_50, complete_voc, special"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl9OQ6oqvNwe"
      },
      "source": [
        "For each language, save:\n",
        "- monolingual vocabulary (dictionary tok:id and id:tok) for mapping.\n",
        "- monolingual vocabulary (txt) for tokenizer.\n",
        "- monolingual embeddings (npy) for mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcbLR8zbcxQD"
      },
      "source": [
        "special_tokens = set()\n",
        "for ln in all_ln:\n",
        "  voc_dict, voc_dict_inv, tok_to_index, complete_voc, special = extract_vocab_dict(ln)\n",
        "  model = BPEmb(lang=ln, vs=50000, dim=300)\n",
        "  embeddings = model.vectors\n",
        "  emb_to_save = []\n",
        "  for token in voc_dict:\n",
        "    emb_to_save.append(embeddings[tok_to_index[token]])\n",
        "  emb_to_save = np.array(emb_to_save)\n",
        "  print(\"Embs length:\", len(emb_to_save))\n",
        "  np.save(MON_EMBEDDINGS_ROOT + ln + \".npy\", emb_to_save)\n",
        "  save_obj(voc_dict, ln,\"/voc_dicts\")\n",
        "  save_obj(voc_dict_inv, ln, \"/voc_dicts_inv\")\n",
        "  # save complete vocabs as txt for the tokenizers (substitute of save_obj(complete_voc, ln, folder = \"NEW/lowercase/BPEmb_map/for_tokenizer_vocabs\"))\n",
        "  with open(OBJECTS_FOLDER + \"/for_tokenizer_vocabs/\" + ln + \".txt\", \"w\") as f:\n",
        "    f.write(\"[PAD]\\n\")\n",
        "    for i in range(100):\n",
        "      f.write(\"[unused\" + str(i) + \"]\\n\")\n",
        "    f.write(\"[UNK]\\n\")\n",
        "    f.write(\"[CLS]\\n\")\n",
        "    f.write(\"[SEP]\\n\")\n",
        "    f.write(\"[MASK]\\n\")\n",
        "    for token in complete_voc:\n",
        "      f.write(token + \"\\n\")\n",
        "  special_tokens = special_tokens.union(set(special))\n",
        "\n",
        "save_obj(special_tokens, \"/special_tokens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWvnGQjEruYB"
      },
      "source": [
        "Now we want to map the monolingual embeddings into the English vector space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtcRYXKnvBfV"
      },
      "source": [
        "def map_embs(embs_src, vocab_src, embs_trg, vocab_trg, output, trans_dict, model, lang_src, lang_trg):\n",
        "\n",
        "  print(\"Loading source embeddings and vocabulary...\")\n",
        "  src_embs = np.load(embs_src)\n",
        "  vocab_src = pickle.load(open(vocab_src,\"rb\"))\n",
        "\n",
        "  print(\"Loading target embeddings and vocabulary...\")\n",
        "  trg_embs = np.load(embs_trg)\n",
        "  vocab_trg = pickle.load(open(vocab_trg,\"rb\"))\n",
        "\n",
        "  if model not in [\"p\", \"b\", \"c\", \"r\"]:\n",
        "    print(\"Error: Unknown mapping/projection model.\")\n",
        "    exit(code = 1)\n",
        "\n",
        "  print(\"Loading translation dictionary...\")\n",
        "  trans_dict = [x.lower().split(\"\\t\") for x in util.load_lines(trans_dict)]\n",
        "\n",
        "  print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" Inducing the mapping and creating a bilingual embedding space\")\n",
        "\n",
        "  if model == \"p\":\n",
        "    embs_src_shared, proj_mat, _ = projection.project_proc(vocab_src, src_embs, vocab_trg, trg_embs, trans_dict)\n",
        "    embs_trg_shared = trg_embs\n",
        "\n",
        "  elif model == \"b\":\n",
        "    embs_src_shared, embs_trg_shared = projection.project_proc_bootstrap(vocab_src, src_embs, vocab_trg, trg_embs, trans_dict)\n",
        "\n",
        "  elif model == \"c\":\n",
        "    embs_src_shared, embs_trg_shared, _ = projection.project_cca(vocab_src, src_embs, vocab_trg, trg_embs, trans_dict)\n",
        "\n",
        "  elif model == \"r\":\n",
        "    embs_src_shared, embs_trg_shared, _ = projection.project_proc_bootstrap_reproduce(vocab_src, src_embs, vocab_trg, trg_embs, trans_dict)\n",
        "    \n",
        "  # save target language (always)\n",
        "  print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" Serializing projected source language embeddings...\")\n",
        "  util.serialize_embs(\"/content/vocab_to_delete\", output + \"vectors_\" + lang_src + \".npy\", vocab_src, embs_src_shared, emb_norm = False, vocab_inv = False)\n",
        "\n",
        "  # save target language (English: do it only once, could also be omitted by copying unmuted embeddings from monolingual)\n",
        "  if lang_src == \"ar\":\n",
        "    print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" Serializing target language embeddings...\")\n",
        "    util.serialize_embs(\"/content/vocab_to_delete\", output + \"vectors_\" + lang_trg + \".npy\", vocab_trg, embs_trg_shared, emb_norm = False, vocab_inv = False)\n",
        "\n",
        "  # if model == \"p\":\n",
        "  #   print(\"Saving projection matrix...\")\n",
        "  #   np.save(output + lang_src + \"-\" + lang_trg + \".proj\", proj_mat)\n",
        "\n",
        "  print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" All done. I'm out of here, ciao bella!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14i0iSD8I5FI"
      },
      "source": [
        "We now map all the embeddings into the English vector space, and save them in the previously created folders: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQyEtF0uKJI"
      },
      "source": [
        "def train (ln_src, ln_trg):\n",
        "  embs_src = OBJECTS_FOLDER + \"/embs/monolingual/vectors_\" + ln_src + \".npy\"\n",
        "  embs_trg = OBJECTS_FOLDER + \"/embs/monolingual/vectors_\" + ln_trg + \".npy\"\n",
        "  vocab_src = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_src + \".pkl\"\n",
        "  vocab_trg = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_trg + \".pkl\"\n",
        "  output = OBJECTS_FOLDER + \"/embs/multilingual/\"\n",
        "  trans_dict = OBJECTS_FOLDER + \"/tsv/\" + ln_src + \"_train.tsv\"\n",
        "  map_embs(embs_src, vocab_src, embs_trg, vocab_trg, output, trans_dict, \"p\", ln_src, ln_trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIl6wi6TvxC3"
      },
      "source": [
        "for ln_src in all_ln:\n",
        "  if ln_src != \"en\":\n",
        "    train(ln_src, \"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ybR-3V0KcWh"
      },
      "source": [
        "# 2. Multilingual Embeddings Evaluation (BLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9wWJfstQfS6"
      },
      "source": [
        "We now evaluate the projected, multilingual embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQMaCAes3f7x"
      },
      "source": [
        "def eval(test_set_path, yacle_embs_src, yacle_embs_trg, vocab_src, vocab_trg, normalize=True, k=0.5, verbose=False):\n",
        "\n",
        "  if verbose:\n",
        "    print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" Deserializing projected source language embeddings...\")\n",
        "  embs_src, norm_embs_src, vocab_dict_src, vocab_dict_inv_src = util.deserialize_embs(vocab_src, yacle_embs_src, emb_norm = False, vocab_inv = False)\n",
        "\n",
        "  if verbose:\n",
        "    print(datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + \" Deserializing target language embeddings...\")\n",
        "  embs_trg, norm_embs_trg, vocab_dict_trg, vocab_dict_inv_trg = util.deserialize_embs(vocab_trg, yacle_embs_trg, emb_norm = False, vocab_inv = False)\n",
        "\n",
        "  positions = []\n",
        "  eval_dict_pairs = [x.lower().split(\"\\t\") for x in util.load_lines(test_set_path)]\n",
        "\n",
        "  for ep in eval_dict_pairs:\n",
        "    if normalize:\n",
        "      ind = sims.most_similar_index(ep[0].strip(), ep[1].strip(), vocab_dict_src, vocab_dict_trg, norm_embs_src, norm_embs_trg) \n",
        "    else:\n",
        "      ind = sims.most_similar_index(ep[0].strip(), ep[1].strip(), vocab_dict_src, vocab_dict_trg, embs_src, embs_trg, cosine_penality=True, k=k) \n",
        "    if ind:\n",
        "      positions.append(ind)\n",
        "\n",
        "  p1 = len([p for p in positions if p == 1]) / len(positions)\n",
        "  p5 = len([p for p in positions if p <= 5]) / len(positions)\n",
        "  p10 = len([p for p in positions if p <= 10]) / len(positions)\n",
        "  p100 = len([p for p in positions if p <= 100]) / len(positions)\n",
        "  mrr = sum([1.0/p for p in positions]) / len(positions)\n",
        "\n",
        "  if verbose:\n",
        "    print(\"Pairs evaluated: \" + str(len(positions)))\n",
        "    print(positions)\n",
        "    print(\"P1: \" + str(p1))\n",
        "    print(\"P5: \" + str(p5))\n",
        "    print(\"P10: \" + str(p10))\n",
        "    print(\"P100: \" + str(p100))\n",
        "    print(\"MRR: \" + str(mrr))\n",
        "  return p1, p5, p10, p100, mrr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgMFSJoczY10"
      },
      "source": [
        "def evaluate_embs(ln_src, ln_trg, verbose=False, inv=False):\n",
        "  print(\"\\n\\n%s to %s\" %(ln_src, ln_trg))\n",
        "  addit = ''\n",
        "  test_lng = ln_src\n",
        "  if inv:\n",
        "    addit = '_inv'\n",
        "    test_lng = ln_trg\n",
        "  # same test file no matter the embeddings type\n",
        "  test_file = OBJECTS_FOLDER + '/tsv/' + test_lng +'_test' + addit + '.tsv'\n",
        "  vocab_src = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_src + \".pkl\"\n",
        "  vocab_trg = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_trg + \".pkl\"\n",
        "  vectors_src = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_src + \".npy\"\n",
        "  vectors_trg = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_trg + \".npy\"\n",
        "\n",
        "  return eval(test_file, vectors_src, vectors_trg, vocab_src, vocab_trg, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6glRyXkthYu"
      },
      "source": [
        "We can directly create a latex table to show results. You will find it in the OBJECTS_FOLDER/BLI_reults.txt file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg6ab6Wb6-aQ"
      },
      "source": [
        "def update_avg(p1_avg_sID, p5_avg_sID, p10_avg_sID, mrr_avg_sID, p1_sID, p5_sID, p10_sID, mrr_sID):\n",
        "      p1_avg_sID += p1_sID\n",
        "      p5_avg_sID += p5_sID\n",
        "      p10_avg_sID += p10_sID\n",
        "      mrr_avg_sID += mrr_sID\n",
        "      return p1_avg_sID, p5_avg_sID, p10_avg_sID, mrr_avg_sID\n",
        "\n",
        "with open(OBJECTS_FOLDER + \"/BLI_reults.txt\", 'w') as res:\n",
        "  \n",
        "  p1_avg_map = p5_avg_map = p10_avg_map = mrr_avg_map = 0\n",
        "\n",
        "  for ln in all_ln:\n",
        "    if ln != 'en':\n",
        "      # ln to en\n",
        "      p1_map, p5_map, p10_map, p100_map, mrr_map = evaluate_embs(ln, 'en', verbose=False, inv=False)\n",
        "      map_res = ln+r\"$\\rightarrow$en &\" + \"{:.3f}\".format(p1_map) + \"&\" + \"{:.3f}\".format(p5_map) + \"&\" + \"{:.3f}\".format(p10_map) + \"&\" + \"{:.3f}\".format(mrr_map)\n",
        "      p1_avg_map, p5_avg_map, p10_avg_map, mrr_avg_map = update_avg(p1_avg_map, p5_avg_map, p10_avg_map, mrr_avg_map, p1_map, p5_map, p10_map, mrr_map)\n",
        "      res.write( map_res + r\"\\\\ \\hline\" + \"\\n\")\n",
        "      \n",
        "      # en to ln\n",
        "      p1_map, p5_map, p10_map, p100_map, mrr_map = evaluate_embs('en', ln, verbose=False, inv=True)\n",
        "      map_res = r\"en$\\rightarrow$\"+ln+\" &\" + \"{:.3f}\".format(p1_map) + \"&\" + \"{:.3f}\".format(p5_map) + \"&\" + \"{:.3f}\".format(p10_map) + \"&\" + \"{:.3f}\".format(mrr_map)\n",
        "      p1_avg_map, p5_avg_map, p10_avg_map, mrr_avg_map = update_avg(p1_avg_map, p5_avg_map, p10_avg_map, mrr_avg_map, p1_map, p5_map, p10_map, mrr_map)\n",
        "      res.write( map_res + r\"\\\\ \\hline\" +\"\\n\")\n",
        "      \n",
        "  pairs = (len(all_ln) - 1) * 2\n",
        "  p1_avg_map /= pairs\n",
        "  p5_avg_map /= pairs\n",
        "  p10_avg_map /= pairs\n",
        "  mrr_avg_map /= pairs\n",
        "  res.write(r\"\\\\ avg &\" + \"{:.3f}\".format(p1_avg_map) + \"&\" + \"{:.3f}\".format(p5_avg_map) + \"&\"\n",
        "        + \"{:.3f}\".format(p10_avg_map) + \"&\" + \"{:.3f}\".format(mrr_avg_map) \n",
        "        + r\"\\\\ \\hline\" +\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbbLs7mVhlmp"
      },
      "source": [
        "# 3. Sparse Translation Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJUjpukAJVoV"
      },
      "source": [
        "In this section we create a translation graph, starting from multilingual embeddings.\n",
        "\n",
        "We first created a sparse graph containing for each subword the top-k translations in each language.\n",
        "\n",
        "The top-k translation of a subword in a source language were found as its k nearest neighbours among the subwords in the target language.\n",
        "\n",
        "We therefore eliminated from the sparse translations graphs all non-symmetric edges, so that the obtained graph contained the edge between \"subword_1\" and \"subword_2\" if and only if \"subword_1\" is among the top-k translations of \"subword_2\" and vice-versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VClGoZFOt6FI"
      },
      "source": [
        "## 3.1. save the edges to files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WpcRen6xxq"
      },
      "source": [
        "Here we test the monolingual embeddings to see if everything is correct:\n",
        "* they are unique\n",
        "* the results of the knn search make sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7S5AWSJSMLA",
        "outputId": "59d51aa4-d39c-4a84-cfb3-bdb3f4092577"
      },
      "source": [
        "# defining desired embeddings\n",
        "ln_src = \"en\"\n",
        "ln_trg = \"en\"\n",
        "\n",
        "# loading embeddings and vocabulary\n",
        "vocab_path_src = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_src + \".pkl\"\n",
        "emb_path_src = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_src + \".npy\"\n",
        "embs = np.load(emb_path_src)\n",
        "voc = load_obj(ln_src, folder = \"/voc_dicts\")\n",
        "tok_to_emb = {}\n",
        "for tok, index in voc.items():\n",
        "  tok_to_emb[tok] = embs[index]\n",
        "\n",
        "# uniqness analysis\n",
        "hashed = [token for token in voc.keys()  if token[:2] == \"##\"]\n",
        "unhashed = [token for token in voc.keys()  if token[:2] != \"##\"]\n",
        "print(\"Check that all embeddings are unique:\\n\")\n",
        "print(\"%d unhashed embeddings, %d unique\" %(len(unhashed), len(set([tuple(tok_to_emb[token]) for token in unhashed]))))\n",
        "print(\"%d hashed embeddings, %d unique\" %(len(hashed), len(set([tuple(tok_to_emb[token]) for token in hashed]))))\n",
        "\n",
        "\n",
        "# loading embeddings and vocabulary for translation analysis\n",
        "vocab_path_trg = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_trg + \".pkl\"\n",
        "emb_path_trg = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_trg + \".npy\"\n",
        "\n",
        "# performance qualitative analysis\n",
        "embs_src, norm_embs_src, vocab_dict_src, vocab_dict_inv_src = util.deserialize_embs(vocab_path_src, emb_path_src, emb_norm = False, vocab_inv = False)\n",
        "embs_trg, norm_embs_trg, vocab_dict_trg, vocab_dict_inv_trg = util.deserialize_embs(vocab_path_trg, emb_path_trg, emb_norm = False, vocab_inv = False)\n",
        "word = \"rabbit\"\n",
        "print(\"\\nList the 10 NN of rabbit among English words\")\n",
        "sims.most_similar(word, vocab_dict_src, norm_embs_src, norm_embs_trg, vocab_dict_inv_trg, num = 10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check that all embeddings are unique:\n",
            "\n",
            "21369 unhashed embeddings, 21369 unique\n",
            "8595 hashed embeddings, 8595 unique\n",
            "\n",
            "List the 10 NN of rabbit among English words\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rabbit', 1.0),\n",
              " ('rabbits', 0.5593842),\n",
              " ('pig', 0.4967025),\n",
              " ('bunny', 0.479895),\n",
              " ('dog', 0.45399833),\n",
              " ('mouse', 0.4380534),\n",
              " ('goat', 0.43747526),\n",
              " ('cats', 0.43455166),\n",
              " ('deer', 0.42751586),\n",
              " ('hare', 0.4134419)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddvN31AijHV"
      },
      "source": [
        "We slightly modify the mehtod to retrieve most similar words, in order to get indexes instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pDS8BkwioCz"
      },
      "source": [
        "def most_similar(word, vocab_src, src_embs_norm, trg_embs_norm, vocab_trg_inv, num = 100, indexes = True):\n",
        "  if word not in vocab_src:\n",
        "    print(\"Word not found in vocabulary: \" + word)\n",
        "    return None\n",
        "  word_emb = src_embs_norm[vocab_src[word]]\n",
        "  sims = np.dot(word_emb, np.transpose(trg_embs_norm))\n",
        "  inds = np.argsort(sims)[-1 * num :][::-1]\n",
        "  scores = [sims[ind] for ind in inds]\n",
        "  if indexes:\n",
        "    return list(zip(inds, scores))\n",
        "  words = [vocab_trg_inv[ind] for ind in inds]\n",
        "  return list(zip(words, scores))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pvmLitmaC-l"
      },
      "source": [
        "We build our sparse graph, where each edge indicates that two nodes are reciprocal translations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THtuTA-Lfej7"
      },
      "source": [
        "def most_similar_words(ln_src, ln_trg, offsets, min_k, max_k):\n",
        "  \"\"\"\n",
        "  Compute all the tranlsation edges for a pair ln_src --> ln_trg\n",
        "  Parameters\n",
        "  ----------\n",
        "  ln_src : str\n",
        "      The code of the source language\n",
        "  ln_trg : str\n",
        "      The code of the target language\n",
        "  offsets : {str : int}\n",
        "      Index offset for each language\n",
        "  k_words : int\n",
        "      The number of translations to consider for words\n",
        "  k_subwords : int\n",
        "      The number of translations to consider for subwords\n",
        "  \"\"\"\n",
        "\n",
        "  # get the source and target (normalized) embeddings and vocabularies (dictionary and inverted dictionary)\n",
        "  vocab_path_src = OBJECTS_FOLDER + \"/voc_dicts/\" +  ln_src + \".pkl\"\n",
        "  emb_path_src = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_src + \".npy\"\n",
        "  vocab_path_trg = OBJECTS_FOLDER + \"/voc_dicts/\" + ln_trg + \".pkl\"\n",
        "  emb_path_trg = OBJECTS_FOLDER + \"/embs/multilingual/vectors_\" + ln_trg + \".npy\"\n",
        "\n",
        "  embs_src, norm_embs_src, vocab_dict_src, vocab_dict_inv_src = util.deserialize_embs(vocab_path_src, emb_path_src, emb_norm = False, vocab_inv = False)\n",
        "  embs_trg, norm_embs_trg, vocab_dict_trg, vocab_dict_inv_trg = util.deserialize_embs(vocab_path_trg, emb_path_trg, emb_norm = False, vocab_inv = False)\n",
        "\n",
        "  # instantiate the graph expansion to be populated\n",
        "  row = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    row[i] = []\n",
        "  col = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    col[i] = []\n",
        "  data = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    data[i] = []\n",
        "\n",
        "  # exclude loops\n",
        "  first_position = 0\n",
        "  if ln_src == ln_trg:\n",
        "    first_position = 1\n",
        "  # correct indexes\n",
        "  row_offset = offsets[ln_src]\n",
        "  col_offset = offsets[ln_trg]\n",
        "\n",
        "  # for each word, append the candidate translation to the graph components\n",
        "  for word in vocab_dict_src:\n",
        "    if word[:2] == \"##\":\n",
        "      k = max_k\n",
        "    else: \n",
        "      k = max_k\n",
        "    indexes_scores = most_similar(word, vocab_dict_src, norm_embs_src, norm_embs_trg, vocab_dict_inv_trg, num=k, indexes=True)\n",
        "    word_index = vocab_dict_src[word]\n",
        "    # depending on the desired number of top-k tranlation, take 'i' translations and add them to the right entry of the dictionary size_to_rows/cols/data\n",
        "    for i in range(min_k, max_k + 1):\n",
        "      for id_sc in indexes_scores[first_position : i]:\n",
        "        row[i].append(word_index + row_offset)\n",
        "        col[i].append(id_sc[0] + col_offset)\n",
        "        data[i].append(id_sc[1])                                                            # we can also put the distance here and partition the weighted graph, but then we suffer from hubness\n",
        "    if word_index % 2500 == 0:\n",
        "      print(\"\\t\", word_index, \"words processed\")\n",
        "  return row, col, data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJ14Ds_h-JO"
      },
      "source": [
        "def translations_edges(min_k, max_k, include_src=False, threshold=0.2):\n",
        "  \"\"\"\n",
        "  Creates translation edges for all language pairs and merge them into a single list.\n",
        "  Parameters\n",
        "  ----------\n",
        "  k : int\n",
        "      The number of translations to consider\n",
        "  \"\"\"\n",
        "  \n",
        "  # compute the index offset for the graph creation\n",
        "  offsets = {}\n",
        "  offset = 0\n",
        "  for ln in all_ln:\n",
        "    offsets[ln] = offset\n",
        "    voc = load_obj(ln, folder = \"/voc_dicts\")  \n",
        "    offset += len(voc)\n",
        "  print(offsets)\n",
        "\n",
        "  # populate the sparse matrix rows, columns and data\n",
        "  row = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    row[i] = []\n",
        "  col = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    col[i] = []\n",
        "  data = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    data[i] = []\n",
        "\n",
        "  for ln_src in map_ln:\n",
        "    for ln_trg in tqdm(map_ln):\n",
        "      if not ln_src == ln_trg or include_src:\n",
        "        print(\"In progress: %s -> %s...\" % (ln_src, ln_trg)) \n",
        "        r , c , d = most_similar_words(ln_src, ln_trg, offsets, min_k, max_k)\n",
        "        for i in range(min_k, max_k+1):\n",
        "          row[i].extend(r[i])\n",
        "          col[i].extend(c[i])\n",
        "          data[i].extend(d[i])\n",
        "  \n",
        "  edges = dict()\n",
        "  for i in range(min_k, max_k+1):\n",
        "    edges[i] = list(zip(row[i], col[i], data[i]))\n",
        "  return edges"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66zK2Rx5Vgqs"
      },
      "source": [
        "Here we compute and save edges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yf04ST9eAzV"
      },
      "source": [
        "# min_k and max_k are used to optimize the computation for multiple values of k\n",
        "min_k = 4\n",
        "max_k = 5\n",
        "edges_dict = translations_edges(min_k, max_k, include_src=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ1q1GSXEPYX"
      },
      "source": [
        "for i in range(min_k, max_k+1):\n",
        "  save_obj(edges_dict[i], \"edges_k_\" + str(i), folder = \"/translation_graphs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrJCgqBqVnT-"
      },
      "source": [
        "## 3.2. create a graph from edges:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYWQhsc3_oSt"
      },
      "source": [
        "We need to create a joint voc_dict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZzfiAEp_tM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "4e331f7f-1408-441e-d89e-28f36d4b3c3d"
      },
      "source": [
        "joint_voc_dict = {}                                       \n",
        "offset = 0\n",
        "print(\"Languages' offsets:\\n\")\n",
        "for ln in all_ln:\n",
        "  print(ln, offset)\n",
        "  voc = load_obj(ln, folder = \"/voc_dicts\")  \n",
        "  for token in voc.keys():\n",
        "    joint_voc_dict[token + '_' + ln] = voc[token] + offset\n",
        "  offset += len(voc)\n",
        "\n",
        "joint_voc_dict_inv = inverse_dict(joint_voc_dict, unique=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Languages' offsets:\n",
            "\n",
            "ar 0\n",
            "bn 29958\n",
            "en 59918\n",
            "fi 89882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4fcfbeebf9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/voc_dicts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mjoint_voc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9hbodCHwmSb"
      },
      "source": [
        "save_obj(joint_voc_dict, \"joint\",  folder = \"/voc_dicts\")\n",
        "save_obj(joint_voc_dict_inv, \"joint\",  folder = \"/voc_dicts_inv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GgiY553zdxT"
      },
      "source": [
        "And now we want to create a sparse graph from the edges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCKLjPoEnQ9I"
      },
      "source": [
        "def sparse_matrix_from_edges(edges, shape, keep_dist=False):\n",
        "  \"\"\"\n",
        "  Creates a sparse matrix from a list of edges. All values are set to 1.\n",
        "  Parameters\n",
        "  ----------\n",
        "  edges : list((int, int))\n",
        "      List containing edges in format (origin_index, destination_index)\n",
        "  shape: (int, int)\n",
        "      The shape of the sparse matrix associated to the graph to create\n",
        "  \"\"\"\n",
        "  # handle different format of edges: (row, col) vs (row, col data)\n",
        "  if len(edges[0]) == 3:\n",
        "    row, col, dist = zip(*edges)\n",
        "  else:\n",
        "    row, col = zip(*edges)\n",
        "  row = np.array(row)\n",
        "  col = np.array(col)\n",
        "  if keep_dist:\n",
        "    data = np.array(dist)\n",
        "  else:\n",
        "    data = np.ones_like(row)\n",
        "  csc = sparse.csc_matrix((data, (row,col)), shape=shape)\n",
        "  return csc\n",
        "\n",
        "\n",
        "def delete_asymmetric_edges(sparse_matrix):\n",
        "  \"\"\"\n",
        "  Deletes all asymmetric edges from the sparse matrix.\n",
        "  Parameters\n",
        "  ----------\n",
        "  sparse_matrix : scipy.sparse.csc_matrix\n",
        "      The input sparse matrix\n",
        "  \"\"\"\n",
        "  return sparse_matrix.multiply(sparse_matrix.transpose())\n",
        "\n",
        "\n",
        "def sparse_graph_from_edges(edges, shape, draw=False):\n",
        "  \"\"\"\n",
        "  Creates a sparse symmetric graph from a list of edges, after having removed all asymmetric edges\n",
        "  Parameters\n",
        "  ----------\n",
        "  edges : list((int, int))\n",
        "      List containing edges in format (origin_index, destination_index)\n",
        "  shape: (int, int)\n",
        "      The shape of the sparse matrix associated to the graph to create\n",
        "  draw: Boolean (default=False)\n",
        "      Whether to draw the created graph. Only set to True when testing on a small edges list\n",
        "  \"\"\"\n",
        "  sparse_matrix = sparse_matrix_from_edges(edges, shape)\n",
        "  symmetric_matrix = delete_asymmetric_edges(sparse_matrix)\n",
        "  sparse_graph = nx.from_scipy_sparse_matrix(symmetric_matrix)\n",
        "  if draw:\n",
        "    nx.draw(sparse_graph, with_labels=True, font_weight='bold')\n",
        "  return sparse_graph"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwE-_69Z7Kps"
      },
      "source": [
        "We want to compute the cliques in order to evaluate graphs for different values of k: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wkgj0lz9skc"
      },
      "source": [
        "def find_cliques(k_namefile, return_list=True):\n",
        "  \"\"\"\"\n",
        "  Find the cliques and communites of a graph.\n",
        "  \"\"\"\n",
        "  print(\"loading edges...\")\n",
        "  voc_size = len(load_obj(\"joint\", folder = \"/voc_dicts\"))\n",
        "  edges = load_obj(\"edges_k_\" + k_namefile, folder = \"/translation_graphs\")\n",
        "  print(\"voc size: \", voc_size)\n",
        "  print(\"len edges: \", len(edges))\n",
        "  print(\"building sparse graph...\")\n",
        "  sparse_graph = sparse_graph_from_edges(edges, (voc_size, voc_size), draw=False)\n",
        "  print(\"findings cliques...\")\n",
        "  cliques = nx.find_cliques_recursive(sparse_graph)\n",
        "  print(\"finding communities...\")\n",
        "  communities = nx.algorithms.community.kclique.k_clique_communities(sparse_graph, 3)\n",
        "  print(\"generating list...\")\n",
        "  if return_list:\n",
        "    return list(cliques), list(communities), sparse_graph\n",
        "  else:\n",
        "    return cliques, communities, sparse_graph\n",
        "\n",
        "\n",
        "def get_cliques_members(cliques):\n",
        "  \"\"\"\n",
        "  Returns a dictionary mapping each cliques size to the set of tokens appearing in at least a clique of that size\n",
        "  \"\"\"\n",
        "\n",
        "  cliques_members_list = defaultdict(list)\n",
        "  for cl in cliques:\n",
        "    cliques_members_list[len(cl)].append(cl)\n",
        "\n",
        "  cliques_members = {}\n",
        "  for length in cliques_members_list.keys():\n",
        "    x = cliques_members_list[length]\n",
        "    y = set().union(*x)\n",
        "    cliques_members[length] = y\n",
        "  cliques_members = OrderedDict(sorted(cliques_members.items()))\n",
        "  return cliques_members\n",
        "\n",
        "\n",
        "def get_cliques_size_frequencies(cliques):\n",
        "  \"\"\"\n",
        "  Returns a dictionary mapping each cliques size to the number of (possibly overlapping) cliques of that size\n",
        "  \"\"\"\n",
        "  cliques_size_frequency = defaultdict(lambda : 0)            \n",
        "  for cl in cliques:\n",
        "    cliques_size_frequency[len(cl)] += 1\n",
        "  cliques_size_frequency = OrderedDict(sorted(cliques_size_frequency.items()))\n",
        "  return cliques_size_frequency\n",
        "\n",
        "\n",
        "def get_cliques_by_size(cliques):\n",
        "  \"\"\"\n",
        "  Returns a dictionary mapping each cliques size to the list of cliques of that size\n",
        "  \"\"\"\n",
        "  size_to_cliques = defaultdict(list)\n",
        "  for cl in cliques:\n",
        "    size_to_cliques[len(cl)].append(cl)\n",
        "  size_to_cliques = OrderedDict(sorted(size_to_cliques.items()))\n",
        "  return size_to_cliques"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryHk9Uq4w3Pr"
      },
      "source": [
        "We save the cliques to pkl:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Emvrcn8gZL"
      },
      "source": [
        "cliques_dict = {}\n",
        "for k in range(min_k, max_k+1):\n",
        "  cliques_dict[k] = find_cliques(str(k), return_list=True)\n",
        "  save_obj(cliques_dict[k], \"k_\" + str(k), folder= \"/translation_graphs/cliques\")\n",
        "  print(\"k=%d saved\" %(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LabtYEW2w6mk"
      },
      "source": [
        "We want to compute, for each clique size, how many tokens belong to a clique of at most that size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeEEtWUYtFEQ"
      },
      "source": [
        "def get_cliques_stats(k, sizes_to_consider, hashed=False):\n",
        "  \"\"\"\n",
        "  Return maximal memmbers for each cliques size, to be plotted with bars().\n",
        "  -------------\n",
        "  Parameters\n",
        "  k: int\n",
        "    the number of translation per token per language to consider, determines the file from which edges are loaded\n",
        "  sizes_to_consider: int\n",
        "    the cliques sizes to plot.\n",
        "  hashed: Boolean\n",
        "    If true, the number of hashed memebers are returned, otherwise the number of members (general).\n",
        "  \"\"\"\n",
        "  y_values = []\n",
        "  # load the first element --> the cliques\n",
        "  cliques = load_obj(\"k_\" + str(k), folder=\"/translation_graphs/cliques\")[0]\n",
        "  print(\"getting clique members...\")\n",
        "  try:\n",
        "    cliques_members = load_obj(\"cliques_members_k_\" + str(k), folder=\"/translation_graphs\")\n",
        "  except:\n",
        "    cliques_members = get_cliques_members(cliques)\n",
        "    save_obj(cliques_members, \"cliques_members_k_\" + str(k), folder=\"/translation_graphs\")\n",
        "\n",
        "  joint_voc_inv = load_obj(\"joint\", folder=\"/voc_dicts_inv\")\n",
        "  all_sizes = list(cliques_members.keys())\n",
        "  sizes = all_sizes[: sizes_to_consider]\n",
        "  max_size = max(all_sizes)\n",
        "  all = 0\n",
        "  print(\"getting maximal members...\")\n",
        "  for size in sizes:\n",
        "    maximal_members = cliques_members[size]\n",
        "    for higher_size in range(size + 1, max_size + 1):\n",
        "      if higher_size in all_sizes:\n",
        "        maximal_members = maximal_members - cliques_members[higher_size]\n",
        "    if not hashed:\n",
        "      y_values.append(len(maximal_members))\n",
        "    else:\n",
        "      hashed = 0\n",
        "      # tok_by_lang = defaultdict(lambda:0)\n",
        "      # hashed_tok_by_lang = defaultdict(lambda:0) \n",
        "      tokens = [joint_voc_inv[index] for index in list(maximal_members)]\n",
        "      for token in tokens:\n",
        "        # tok_by_lang[token[-2:]] += 1\n",
        "        if token[:2] == \"##\":\n",
        "          hashed += 1\n",
        "          # hashed_tok_by_lang[token[-2:]] += 1\n",
        "      y_values.append(hashed)\n",
        "\n",
        "  return y_values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENOk8iV8zcY3"
      },
      "source": [
        "Helper function to plot multiple bars:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kVItDh1Ch1y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def bar_plot(ax, data, x_len=5, colors=None, total_width=0.8, single_width=1, legend=True):\n",
        "    # Check if colors where provided, otherwhise use the default color cycle\n",
        "    if colors is None:\n",
        "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "    # Number of bars per group\n",
        "    n_bars = len(data)\n",
        "\n",
        "    # The width of a single bar\n",
        "    bar_width = total_width / n_bars\n",
        "\n",
        "    # List containing handles for the drawn bars, used for the legend\n",
        "    bars = []\n",
        "\n",
        "    # Iterate over all data\n",
        "    for i, (name, values) in enumerate(data.items()):\n",
        "        # The offset in x direction of that bar\n",
        "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
        "\n",
        "        # Draw a bar for every value of that type\n",
        "        for x, y in enumerate(values):\n",
        "            bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])\n",
        "\n",
        "        # Add a handle to the last drawn bar, which we'll need for the legend\n",
        "        bars.append(bar[0])\n",
        "\n",
        "    # plt.xticks(range(x_len), [\"k = \" + str(i) for i in range(1, x_len+1)])\n",
        "    plt.xticks(range(x_len), [\"k=\" + str(i) for i in range(1, x_len+1)])\n",
        "\n",
        "    # Draw legend if we need\n",
        "    if legend:\n",
        "        ax.legend(bars, data.keys())\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgbtYe4B5Vhg"
      },
      "source": [
        "We show how k=5 is a good value, since it is the smallest value with a reasonably low number of singletons:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j3dxe1Afxk7"
      },
      "source": [
        "min_k=1\n",
        "max_k=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmeMECE3Fk2p"
      },
      "source": [
        "x = [1,2,3,4,5]\n",
        "\n",
        "members_1 = []\n",
        "members_2 = []\n",
        "members_3 = []\n",
        "members_4 = []\n",
        "members_5 = []\n",
        "members_6 = []\n",
        "members_7 = []\n",
        "members_8 = []\n",
        "members_9 = []\n",
        "\n",
        "# run it also with hashed=True to display hashed tokens stats\n",
        "for k in range(min_k, max_k + 1):\n",
        "  if k!=3:\n",
        "    try:\n",
        "      a,b,c,d,e,f,g,h,i = get_cliques_stats(k, 9, hashed=False)\n",
        "    except:\n",
        "      a,b,c,d,e,f,g,h = get_cliques_stats(k, 9, hashed=False)\n",
        "      i = 0\n",
        "  else:\n",
        "    a,b,c,d,e,f,g,h,i = 0,0,0,0,0,0,0,0,0\n",
        "  print(a,b,c,d,e,f,g,h,i)\n",
        "  members_1.append(a)\n",
        "  members_2.append(b)\n",
        "  members_3.append(c)\n",
        "  members_4.append(d)\n",
        "  members_5.append(e)\n",
        "  members_6.append(f)\n",
        "  members_7.append(g)\n",
        "  members_8.append(h)\n",
        "  members_9.append(i)\n",
        "  print(k, \"done...\")\n",
        "\n",
        "data = {\n",
        "    \"members_1\": members_1,\n",
        "    \"members_2\": members_2,\n",
        "    \"members_3\": members_3,\n",
        "    \"members_4\": members_4,\n",
        "    \"members_5\": members_5,\n",
        "    \"members_6\": members_6,\n",
        "    \"members_7\": members_7,\n",
        "    \"members_8\": members_8,\n",
        "    \"members_9\": members_9}\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_ylabel('tokens', fontsize=\"large\")\n",
        "ax.set_xlabel('clique size across different values of k', fontsize=\"large\")\n",
        "bar_plot(ax, data, colors=None, total_width=.8, single_width=.9)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhurD2ToDS9i"
      },
      "source": [
        "We choose k = 5, and we get a dictionary containing cliques by size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogF6t4ihDUzI"
      },
      "source": [
        "cliques = load_obj(\"k_5\", folder = \"/translation_graphs/cliques\")[0]\n",
        "size_to_cliques = get_cliques_by_size(cliques)\n",
        "print(size_to_cliques.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZqUZH-I5siL"
      },
      "source": [
        "Here we print a table showing the average number of languages per clique, by clique size (text format and latex format):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YQrieo89bWb"
      },
      "source": [
        "joint_voc_inv = load_obj(\"joint\", folder=\"/voc_dicts_inv\")\n",
        "table = \"\"\n",
        "for size in range(1, len(list(size_to_cliques.keys())) + 1):\n",
        "  number_of_cliques = len(size_to_cliques[size])\n",
        "  avg_number_of_languages = 0\n",
        "  for clique_of_size in size_to_cliques[size]:\n",
        "    ln_number = len(set([joint_voc_inv[index][-2:] for index in clique_of_size]))\n",
        "    avg_number_of_languages += ln_number\n",
        "  avg_number_of_languages /= number_of_cliques\n",
        "  print(\"Cliques of size {:2} on average contain tokens from {:.2f} different languages\".format(size, avg_number_of_languages))\n",
        "  table += (str(size) + \" & \" + \"{:.2f}\".format(avg_number_of_languages) + \" & \" + str(len(size_to_cliques[size])) + \"\\\\\\\\ \\\\hline \\n\")\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1do9zdGhGyX5"
      },
      "source": [
        "We want to see how many transaltions tokens in cliques of size 2 have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8EUv85rHA7T"
      },
      "source": [
        "cliques_members = load_obj(\"cliques_members_k_5\", folder= \"/translation_graphs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLcv0fh-YF-t"
      },
      "source": [
        "def maximal_members(cliques_members, size):\n",
        "  all_sizes = list(cliques_members.keys())\n",
        "  max_size = max(all_sizes)\n",
        "  maximal_members = cliques_members[size]\n",
        "  for higher_size in range(size + 1, max_size + 1):\n",
        "    if higher_size in all_sizes:\n",
        "      maximal_members = maximal_members - cliques_members[higher_size]\n",
        "  return maximal_members"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX797HUkJaLx"
      },
      "source": [
        "maximal_members_2 = maximal_members(cliques_members, 2)\n",
        "len(maximal_members_2)\n",
        "len(joint_voc_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWtL4o01jFO"
      },
      "source": [
        "voc_size = len(load_obj(\"joint\", folder = \"/voc_dicts\"))\n",
        "edges = load_obj(\"edges_k_5\", folder = \"/translation_graphs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pLlnTwCiVa"
      },
      "source": [
        "sparse_matrix = sparse_matrix_from_edges(edges, (voc_size, voc_size))\n",
        "symmetric_matrix = delete_asymmetric_edges(sparse_matrix)\n",
        "rows_indexes = list(symmetric_matrix.nonzero()[0])\n",
        "print(\"row indexes:\", len(rows_indexes))\n",
        "id_to_translations_2 = defaultdict(lambda : 0)\n",
        "for id in rows_indexes:\n",
        "  if id in maximal_members_2:\n",
        "    id_to_translations_2[id] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYhcgBxMXgY0"
      },
      "source": [
        "translations_frequencies = defaultdict(lambda : 0)\n",
        "for id, n_trans in id_to_translations_2.items():\n",
        "  translations_frequencies[n_trans] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlkXVGfAY691"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = list(translations_frequencies.keys())\n",
        "y = list(translations_frequencies.values())\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"\\\"strong translations\\\"\", fontsize=\"large\")\n",
        "plt.ylabel(\"tokens\", fontsize=\"large\")\n",
        "# plt.title(\"Number of strong (bidirectional) translations for subwords only occurring in cliques of size 2\")     title added in latex\n",
        "# data = {\"subgraphs\" : y}\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
        "# fig, ax = plt.subplots()\n",
        "# bar_plot(ax, data, x_len=len(x), colors=None, total_width=.8, single_width=.9)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxD7V_ATx4_M"
      },
      "source": [
        "Let's check if we have big objects in memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h-k_63wEcsS"
      },
      "source": [
        "import sys\n",
        "\n",
        "local_vars = list(locals().items())\n",
        "for var, obj in local_vars:\n",
        "  if sys.getsizeof(obj) > 100000:\n",
        "    print(var, sys.getsizeof(obj))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdWqTFBWZxO8"
      },
      "source": [
        "# 4. Graph Partitioning (i.e. clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_V6ks3kC-Z6"
      },
      "source": [
        "In this section, we partition the created graph with the [METIS alogrithm](http://glaros.dtc.umn.edu/gkhome/metis/metis/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpdH8R_MfOHM"
      },
      "source": [
        "Install nxmetis.\n",
        "\n",
        "**Note**: it is necessary that the nxmetis package is in the icebert_clustering folder. Copy it from the [github repo](https://github.com/BassaniRiccardo/transformers/tree/master/examples/pytorch/language-modeling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbNeij1WJtIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4059e07a-8bdb-4028-a2fc-9aae6b413185"
      },
      "source": [
        "% cd /content/drive/MyDrive/notebooks/icebert_clustering\n",
        "% cd networkx-metis\n",
        "! python setup.py install\n",
        "! pip install cython\n",
        "import nxmetis"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/notebooks/icebert_clustering\n",
            "/content/drive/MyDrive/notebooks/icebert_clustering/networkx-metis\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing networkx_metis.egg-info/PKG-INFO\n",
            "writing dependency_links to networkx_metis.egg-info/dependency_links.txt\n",
            "writing requirements to networkx_metis.egg-info/requires.txt\n",
            "writing top-level names to networkx_metis.egg-info/top_level.txt\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE.txt'\n",
            "adding license file 'NOTICE'\n",
            "writing manifest file 'networkx_metis.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running build_clib\n",
            "building 'gklib' library\n",
            "building 'metis' library\n",
            "running install_lib\n",
            "running build_py\n",
            "running build_ext\n",
            "building 'nxmetis._metis' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Isrc/GKlib -Isrc/libmetis -I/usr/include/python3.7m -c nxmetis/_metis.c -o build/temp.linux-x86_64-3.7/nxmetis/_metis.o\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/nxmetis/_metis.o -Lbuild/temp.linux-x86_64-3.7 -lmetis -lgklib -lgklib -lmetis -o build/lib.linux-x86_64-3.7/nxmetis/_metis.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/_metis.cpython-37m-x86_64-linux-gnu.so -> nxmetis\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/nxmetis\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/_metis.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "creating build/bdist.linux-x86_64/egg/nxmetis/tests\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/tests/test_metis.py -> build/bdist.linux-x86_64/egg/nxmetis/tests\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/tests/__init__.py -> build/bdist.linux-x86_64/egg/nxmetis/tests\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/__init__.py -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/enums.py -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/exceptions.py -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/metis.py -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "copying build/lib.linux-x86_64-3.7/nxmetis/types.py -> build/bdist.linux-x86_64/egg/nxmetis\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/tests/test_metis.py to test_metis.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/tests/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/enums.py to enums.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/exceptions.py to exceptions.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/metis.py to metis.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/types.py to types.cpython-37.pyc\n",
            "creating stub loader for nxmetis/_metis.cpython-37m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nxmetis/_metis.py to _metis.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying networkx_metis.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying networkx_metis.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying networkx_metis.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying networkx_metis.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying networkx_metis.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "nxmetis.__pycache__._metis.cpython-37: module references __file__\n",
            "creating 'dist/networkx_metis-1.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing networkx_metis-1.0-py3.7-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/networkx_metis-1.0-py3.7-linux-x86_64.egg\n",
            "Extracting networkx_metis-1.0-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding networkx-metis 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/networkx_metis-1.0-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for networkx-metis==1.0\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for decorator==4.4.2\n",
            "Best match: decorator 4.4.2\n",
            "Adding decorator 4.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for networkx==2.5.1\n",
            "Best match: networkx 2.5.1\n",
            "Adding networkx 2.5.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for networkx-metis==1.0\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4l3UkGMQPe"
      },
      "source": [
        "# ! git clone https://github.com/networkx/networkx-metis.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw2FsSKMfTLz"
      },
      "source": [
        "Test it is working:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEOYXx23Mj2L",
        "outputId": "b9b432ca-8c5e-4dc0-9262-48cf2f685c84"
      },
      "source": [
        "G = nx.complete_graph(10)\n",
        "import nxmetis\n",
        "nxmetis.partition(G, 2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, [[0, 1, 2, 3, 6], [4, 5, 7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTEqCmbyfVuz"
      },
      "source": [
        "Apply it to the graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztKMJZ_8tYoZ"
      },
      "source": [
        "def metis_balanced_partitioning(nparts, ufactor, k_namefile=\"5\"):\n",
        "  \"\"\"\n",
        "  Find the best partition of a graph.\n",
        "  \"\"\"\n",
        "  print(\"loading edges...\")\n",
        "  voc_size = len(load_obj(\"joint\", folder = \"/voc_dicts\"))\n",
        "  edges = load_obj(\"edges_k_\" + k_namefile, folder = \"/translation_graphs\")\n",
        "  print(\"building sparse graph...\")\n",
        "  sparse_graph = sparse_graph_from_edges(edges, (voc_size, voc_size), draw=False)\n",
        "  print(\"partitioning...\")\n",
        "  objval, partitions = nxmetis.partition(sparse_graph, nparts, options=nxmetis.MetisOptions(ufactor=ufactor))\n",
        "  return objval, partitions"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTg8XMrfb1D"
      },
      "source": [
        "Check how many dedicated clusters are needed for the special tokens, in order to set nparts=30000-len(special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aTOWPrdNMyt",
        "outputId": "acbe4381-ff6d-4219-9633-88ebde6898e3"
      },
      "source": [
        "special_tokens = load_obj(\"/special_tokens\")\n",
        "len(special_tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gf3B7lCLN3_"
      },
      "source": [
        "We set k=28500 since we have to keep 1228 dedicated clusters, 665 for special tokens, but some subgraphs will be empty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vElEgr5pOBjH",
        "outputId": "3e7b1843-e1c1-413f-89f9-f6fbf7313292"
      },
      "source": [
        "v = 1.5\n",
        "k = 28500 \n",
        "ufactor = int(1000 * v)\n",
        "print(\"\\nv =\", v)\n",
        "objval, partitions = metis_balanced_partitioning(k, ufactor)\n",
        "save_obj(partitions, \"partition_v_1.5\", folder=\"/translation_graphs/partitions\")  # partitions = load_obj(\"partition_v_1.5\", folder=\"/translation_graphs/partitions\")   if ready"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "v = 1.5\n",
            "loading edges...\n",
            "building sparse graph...\n",
            "partitioning...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8MZVM05FZlD",
        "outputId": "042349b5-8ee7-4bcc-8e09-b7b0e2e3da27"
      },
      "source": [
        "partition_sizes = defaultdict(lambda : 0)\n",
        "for p in partitions:\n",
        "  partition_sizes[len(p)] += 1\n",
        "partition_sizes = OrderedDict(sorted(partition_sizes.items()))\n",
        "print(partition_sizes)\n",
        "print(\"\\n\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(0, 50), (1, 1), (2, 15), (3, 37), (4, 22), (5, 22), (6, 19), (7, 434), (8, 17454), (9, 6127), (10, 840), (11, 442), (12, 321), (13, 224), (14, 177), (15, 161), (16, 122), (17, 127), (18, 104), (19, 96), (20, 92), (21, 101), (22, 98), (23, 1414)])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woro4HyzylAI"
      },
      "source": [
        "Plot the distribution of clusters' sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "hai009X-9ZKB",
        "outputId": "169a3729-9a6a-4588-8076-96e318706dde"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, y = zip(*list(partition_sizes.items()))\n",
        "x_len=len(x)\n",
        "plt.bar(x, y)\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"group size\", fontsize=\"large\")\n",
        "plt.ylabel(\"number of groups\", fontsize=\"large\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKIE63aAMX8Q"
      },
      "source": [
        "Some post-processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv5ceUuHZ2wn",
        "outputId": "c9c7118c-fdfb-4a98-e0ad-ba9086b47f9c"
      },
      "source": [
        "print(\"We have\", len(partitions), \"partitions.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 28500 partitions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2EuhnOgV7Z"
      },
      "source": [
        " Remove isolated words, and delete empty partitions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKcxDEHmYp8t",
        "outputId": "a86f2ce3-a792-446a-e1d8-8fe5d1c5e815"
      },
      "source": [
        "cliques_members = load_obj(\"cliques_members_k_5\", folder=\"/translation_graphs\")\n",
        "to_remove = maximal_members(cliques_members, 1)\n",
        "print(len(to_remove), \"singletons to remove.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1228 singletons to remove.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJbCszcZelk"
      },
      "source": [
        "# we remove empty partitions, included those which resulted empty after we removed a SINGLETONS (i.e. a token which we KNOW MUST BE ALONE)\n",
        "corrected_partitions = []\n",
        "for p in partitions:\n",
        "  group = p\n",
        "  # group = list(set(p) - set(to_remove))\n",
        "  if len(group) > 0:\n",
        "    corrected_partitions.append(group)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrDIUVchaZq_",
        "outputId": "ae0e70e6-a15a-4dbe-fe56-ff75d72933e9"
      },
      "source": [
        "print(\"After removing empty partitions and isolated words we have\", len(corrected_partitions), \"partitions.\")\n",
        "singletons_list = [[s] for s in to_remove]\n",
        "corrected_partitions.extend(singletons_list)\n",
        "print(\"After adding the singletons, we have\", len(corrected_partitions), \"partitions.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removing empty partitions and isolated words we have 28450 partitions.\n",
            "After adding the singletons, we have 29678 partitions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX4GXX9GhkQ1"
      },
      "source": [
        "We may still have some clusters containing words that are not connected in the graph, due to the nature of the used partition algorithm. We investigate this phenomenon analyzing the average intra-cluster path distance, looking at all combinations of tokens belonging to the same non-singleton cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezycX6JfOvdK",
        "outputId": "5d100d56-facf-43e3-8121-bc78736939c5"
      },
      "source": [
        "partition = corrected_partitions\n",
        "sparse_graph = load_obj(\"k_5\", folder=\"/translation_graphs/cliques\")[2]\n",
        "\n",
        "print(\"checking connections...\")\n",
        "path_lengths = defaultdict(lambda : 0)\n",
        "no_path = 0\n",
        "\n",
        "for pn, group in enumerate(partition):\n",
        "  if len(group) > 1:\n",
        "    import itertools\n",
        "    pairs = list(itertools.combinations(group, r=2))\n",
        "    for p in pairs:\n",
        "      try:\n",
        "        d = nx.shortest_path_length(sparse_graph, p[0], p[1])\n",
        "        path_lengths[d] += 1\n",
        "      except:\n",
        "        no_path += 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checking connections...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCMB1Q54Nf67"
      },
      "source": [
        "path_lengths = OrderedDict(sorted(path_lengths.items()))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBqLUozPPTyG",
        "outputId": "88cf4a16-c3a0-45d9-8653-b2851c13d6f7"
      },
      "source": [
        "x = [str(l) for l in list(path_lengths.keys())]\n",
        "x.append(\"no_path\")\n",
        "y = list(path_lengths.values())\n",
        "y.append(no_path)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '2', '3', '4', '5', '6', '7', '8', 'no_path']\n",
            "[454540, 497528, 228574, 99287, 39431, 8958, 642, 17, 7274]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "P5a_B0iBg9ok",
        "outputId": "2274df56-e3c0-43e7-9ba6-f320a29f86bd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"path length\")\n",
        "plt.ylabel(\"number of pairs\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "\n",
        "# plt.yscale('log')               # set scale\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln-EhOeiiMOs"
      },
      "source": [
        "We now proceed by splitting the clusters made by non-connected tokens into multiple connected groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gZPl0K3WVWV",
        "outputId": "571f8b2a-f7ac-405c-9026-b94a851f5744"
      },
      "source": [
        "partition = corrected_partitions\n",
        "print(\"checking connections...\")\n",
        "\n",
        "islands_to_add = []\n",
        "partition_to_remove = []\n",
        "for p_id, group in enumerate(partition):\n",
        "  if len(group) > 1:\n",
        "    for w in group:\n",
        "      isolated = False\n",
        "      island_members = [w]\n",
        "      for mate in group:\n",
        "        if w != mate:\n",
        "          if not nx.has_path(sparse_graph, w, mate):\n",
        "            isolated = True\n",
        "            if not p_id in partition_to_remove:\n",
        "              partition_to_remove.append(p_id)\n",
        "          else:\n",
        "            island_members.append(mate)\n",
        "      if isolated:\n",
        "        island = sorted(island_members)\n",
        "        if not island in islands_to_add:\n",
        "          islands_to_add.append(island)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checking connections...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ6avLU-kOGN",
        "outputId": "8346730b-9e93-4777-c90f-524bb2bca15a"
      },
      "source": [
        "print(\"to remove:\", len(partition_to_remove))\n",
        "print(\"to add:\", len(islands_to_add))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to remove: 753\n",
            "to add: 1931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuEP2sEjiXxU",
        "outputId": "42c1e26d-9973-4fd7-ea8f-0c7c4715aa64"
      },
      "source": [
        "partition = [partition[i] for i in range(len(partition)) if not i in partition_to_remove]\n",
        "print(\"After removal of non-connected groups, we have\", len(partition), \"partitions.\")\n",
        "partition.extend(islands_to_add)\n",
        "print(\"After adding the connected sub-groups, we have\", len(partition), \"partitions.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removal of non-connected groups, we have 28925 partitions.\n",
            "After adding the connected sub-groups, we have 30856 partitions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMT7QWEecbph"
      },
      "source": [
        "From a partition (list of lists of ids) create a tok_to_cID dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqRIOX_Pa44e"
      },
      "source": [
        "joint_voc_inv = load_obj(\"joint\", folder = \"/voc_dicts_inv\")\n",
        "special_tokens = load_obj(\"/special_tokens\")\n",
        "\n",
        "tok_to_cID = {}\n",
        "for cID, p in enumerate(partition):\n",
        "  for id in p:\n",
        "    tok = joint_voc_inv[id]\n",
        "    tok_to_cID[tok] = cID\n",
        "\n",
        "count = len(partition)\n",
        "for special in special_tokens:\n",
        "  tok_to_cID[special] = count\n",
        "  count += 1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIzAcuwPlcz9"
      },
      "source": [
        "Check that it works correctly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC_H-x2Wc0UT",
        "outputId": "6b1baf7f-dfac-4710-edef-b85805b48da9"
      },
      "source": [
        "cID_to_tok = inverse_dict(tok_to_cID)\n",
        "\n",
        "print(len(tok_to_cID))\n",
        "print(len(cID_to_tok))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "270297\n",
            "30293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kway2TfSIqV"
      },
      "source": [
        "Print some cluster examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5IEyKlC7JkB",
        "outputId": "76636495-28b2-46ff-fee9-86b19ff481c4"
      },
      "source": [
        "for i in range(10):\n",
        "  cID = np.random.randint(len(cID_to_tok))\n",
        "  print(cID_to_tok[cID])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['নান_bn', '##udo_id', '타이_ko', '##타이_ko', '게이오_ko', '##aris_ru', 'anda_sw', 'sar_te', '##va_te']\n",
            "['মিছিল_bn', 'rally_en', 'rallies_en', 'sanoitta_fi', 'sayembara_id', '공모_ko', '집회_ko', 'инициа_ru', '##ungana_sw']\n",
            "['diac_en', 'wireless_id', '##cycl_ko', '##결에_ko', '내비_ko', '##oid_ru', '##tebbe_sw', '##pokeaji_sw']\n",
            "['##zewa_sw', '##pr_sw', 'kusikiti_sw', '##lski_sw', 'ఊ_te', '##రుకు_te', '##రును_te', '##రులో_te', 'పనితీ_te']\n",
            "['##جاعة_ar', 'قلادة_ar', 'فاير_ar', '##বডি_bn', 'কানুন_bn', '강력_ko', '##ంచర్_te', 'మొండి_te']\n",
            "['توي_ar', 'سون_ar', '##জু_bn', '##াতু_bn', '##yo_en', 'yo_en', 'yo_fi', '##yo_fi']\n",
            "['##فقات_ar', '##aine_fi', '##aineena_fi', '##aineita_fi', '##aineiden_fi', '##aineet_fi', 'поступления_ru', 'karanga_sw']\n",
            "['##جاني_ar', '##nal_en', '##landers_en', 'hym_en', '##asta_en', 'haynes_en', 'шот_ru', '##శంకర_te']\n",
            "['##ple_en', '##ples_en', '##rip_en', 'crip_en', '##мел_ru', '##плей_ru', '##స్లో_te', '##స్కు_te', '##స్ను_te']\n",
            "['##ina_en', '##na_en', '##ena_en', '##na_fi', '##ta_id', '##ti_id', '##na_id', '##ni_id', '##ns_sw', '##క్_te']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETTPr-tElf-1"
      },
      "source": [
        "Finally, convert integers in strings and save the mapper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O30D7LX3PdvZ"
      },
      "source": [
        "tok_to_cID_string = {}\n",
        "for k, v in tok_to_cID.items():\n",
        "  tok_to_cID_string[k] = str(v)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uApsR9RHeShi"
      },
      "source": [
        "for special_token in [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"]:\n",
        "  tok_to_cID_string[special_token] = special_token\n",
        "save_obj(tok_to_cID_string, \"/tok_to_cID_string\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ocrGMJ2-ME"
      },
      "source": [
        "# 5. Encoder Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWn6XXvelmwt"
      },
      "source": [
        "Test that the encoder works. This is what is used to map text to cIDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTjD4fPi3S01"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# from pytorch_transformers.tokenization_bert import BertTokenizer\n",
        "ln = \"en\"\n",
        "bt = BertTokenizerFast('/content/drive/MyDrive/notebooks/icebert_clustering/objects/for_tokenizer_vocabs/' + ln + '.txt', do_lower_case=False)\n",
        "\n",
        "def fast_tokenize(text, ln, tokenizer):\n",
        "  text = text.lower()\n",
        "  text = ''.join(\"0\" if c.isdigit() else c for c in text)\n",
        "  tokens = bt.tokenize(text)\n",
        "  for i, t in enumerate(tokens):\n",
        "    tokens[i] = t + '_' + ln            \n",
        "  return tokens\n",
        "\n",
        "def encode_cID(sentence, ln, tok_to_cID, tokenizer, verbose=False):\n",
        "  # tokens = tkn.tokenize(sentence, ln, tokenizer=\"BPEmb\")\n",
        "  tokens = fast_tokenize(sentence, ln, tokenizer)\n",
        "  cIDs = []\n",
        "  for t in tokens:\n",
        "    try:\n",
        "      id = tok_to_cID[t]\n",
        "    except:\n",
        "      try:\n",
        "        id = tok_to_cID[t[:-3]]\n",
        "      except:\n",
        "        id = \"UNK\"\n",
        "    cIDs.append(id)\n",
        "  if verbose:\n",
        "    print(tokens)\n",
        "  return cIDs"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNi76E4AdAwt",
        "outputId": "0d30617a-3557-4211-a7af-81bc5c9e1fd7"
      },
      "source": [
        "tok_to_cID = load_obj(\"/tok_to_cID_string\")\n",
        "tokens = encode_cID(\"Goodmorning world!\", ln, tok_to_cID, bt, verbose=True)\n",
        "print(\" \".join(tokens))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good_en', '##morning_en', 'world_en', '!_en']\n",
            "24289 20375 22825 31218\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}