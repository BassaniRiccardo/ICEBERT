{
	"model_type": "uncased_baseline",
	"train_file" : "path_to_training_corpus",
	"max_seq_length" : 512,
	"continue_training" = 0,

	"icebert_folder" : "/Users/ricca/Thesis/transformers/examples/pytorch/language-modeling/icebert",
	"output_models_folder" : "/Users/ricca/Thesis/transformers/examples/pytorch/language-modeling/output_models",

	"max_train_samples" : 100,
	"max_eval_samples" : 10

}